import streamlit as st
import pdfplumber
from langchain.chat_models import ChatOpenAI

# ãƒšãƒ¼ã‚¸ã®åˆæœŸåŒ–
def init_page():
    st.set_page_config(page_title="å­¦ç¿’ãƒ˜ãƒ«ãƒ‘ãƒ¼", page_icon="ğŸ“˜")
    st.title("PDF å­¦ç¿’æ”¯æ´ (GPT-4)")

# GPT-4ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def initialize_model():
    return ChatOpenAI(temperature=0, model_name="gpt-4o")

# PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
def get_pdf_text(uploaded_file):
    with pdfplumber.open(uploaded_file) as pdf:
        return "\n".join(page.extract_text() for page in pdf.pages)

# GPTã«å•é¡Œä½œæˆã‚’ä¾é ¼ã™ã‚‹é–¢æ•°
def generate_question_with_gpt(llm, pdf_text, question_type):
    prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«åŸºã¥ã„ã¦ã€ä¸€å•ä¸€ç­”å½¢å¼ã®å•é¡Œã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    æ–°ã—ã„å•é¡Œã‚’å§‹ã‚ã‚‹éš›ã¯ã€ã€Œæ¬¡ã®å•é¡Œã€ã¨è¡¨ç¤ºã—ã¦ã‹ã‚‰å•é¡Œã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
    å•é¡Œã¯ä¸€å•ã®ã¿ä½œæˆã—ã¦ãã ã•ã„ã€‚è¤‡æ•°ç­”ãˆã•ã›ã‚‹å•é¡Œã¯ä½œæˆã—ãªã„ã§ãã ã•ã„ã€‚


    ãƒ†ã‚­ã‚¹ãƒˆ:
    {pdf_text}

    å•é¡Œã®ã‚¿ã‚¤ãƒ—: {question_type}

    æœ€åˆã®å•é¡Œã‚’å‡ºé¡Œã—ã¦ãã ã•ã„ã€‚
    """
    return llm.predict(prompt)

# GPTã«æ­£èª¤åˆ¤å®šã‚’ä¾é ¼ã™ã‚‹é–¢æ•°
def check_answer_with_gpt(llm, conversation, user_answer):
    prompt = f"""ä»¥ä¸‹ã®ä¼šè©±ã®ç¶šãã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã«å¯¾ã™ã‚‹æ­£èª¤åˆ¤å®šã¨è©³ç´°ãªè§£èª¬ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
    æ¬¡ã®å•é¡Œã¯å‡ºã•ãšã«ã€æ­£èª¤åˆ¤å®šã¨è§£èª¬ã®ã¿ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

    ã“ã‚Œã¾ã§ã®ä¼šè©±:
    {conversation}

    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”: {user_answer}

    æ­£èª¤åˆ¤å®šã¨è§£èª¬:
    """
    return llm.predict(prompt)

# æ¬¡ã®å•é¡Œã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
def generate_next_question(llm, conversation):
    prompt = f"""ä»¥ä¸‹ã®ä¼šè©±ã®ç¶šãã¨ã—ã¦ã€æ–°ã—ã„å•é¡Œã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
    ã€Œæ¬¡ã®å•é¡Œã€ã¨è¡¨ç¤ºã—ã¦ã‹ã‚‰å•é¡Œã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

    ã“ã‚Œã¾ã§ã®ä¼šè©±:
    {conversation}

    æ¬¡ã®å•é¡Œ:
    """
    return llm.predict(prompt)

def main():
    init_page()  # ãƒšãƒ¼ã‚¸ã‚’åˆæœŸåŒ–
    llm = initialize_model()  # GPT-4ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’åˆæœŸåŒ–
    if 'conversation' not in st.session_state:
        st.session_state.conversation = ""
    if 'current_question' not in st.session_state:
        st.session_state.current_question = ""
    if 'waiting_for_answer' not in st.session_state:
        st.session_state.waiting_for_answer = False
    if 'user_answer' not in st.session_state:
        st.session_state.user_answer = ""

    uploaded_file = st.file_uploader("å‹‰å¼·ç”¨PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ğŸ“š", type='pdf')
    if uploaded_file:
        with st.spinner("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­..."):
            pdf_text = get_pdf_text(uploaded_file)
        st.success("PDFã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸï¼")

        question_type = st.text_input("ã©ã®ã‚ˆã†ãªå•é¡Œã‚’å‡ºã—ã¦ã»ã—ã„ã§ã™ã‹ï¼Ÿï¼ˆä¾‹ï¼šå˜èªã®æ„å‘³ã‚’å•ã†ã€æ–‡æ³•ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ï¼‰")
        if question_type and not st.session_state.current_question:
            st.session_state.current_question = generate_question_with_gpt(llm, pdf_text, question_type)
            st.session_state.conversation += f"\n{st.session_state.current_question}"
            st.session_state.waiting_for_answer = True

        if st.session_state.current_question:
            st.write("ç¾åœ¨ã®å•é¡Œ:")
            st.write(st.session_state.current_question)

        if st.session_state.waiting_for_answer:
            user_answer = st.text_input("ã‚ãªãŸã®å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆçµ‚äº†ã™ã‚‹å ´åˆã¯ã€Œçµ‚äº†ã€ã¨å…¥åŠ›ï¼‰:", key="answer_input")
            if user_answer.lower() == "çµ‚äº†":
                st.write("å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã™ã€‚ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼")
                st.session_state.conversation = ""
                st.session_state.current_question = ""
                st.session_state.waiting_for_answer = False
                st.session_state.user_answer = ""
            elif user_answer:
                st.session_state.conversation += f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”: {user_answer}"
                feedback = check_answer_with_gpt(llm, st.session_state.conversation, user_answer)
                st.write("è©•ä¾¡çµæœ:")
                st.write(feedback)
                st.session_state.conversation += f"\n{feedback}"
                st.session_state.waiting_for_answer = False
                st.session_state.user_answer = user_answer

        if not st.session_state.waiting_for_answer and st.session_state.current_question:
            if st.button("æ¬¡ã®å•é¡Œã¸é€²ã‚€"):
                st.session_state.current_question = generate_next_question(llm, st.session_state.conversation)
                st.session_state.conversation += f"\n{st.session_state.current_question}"
                st.session_state.waiting_for_answer = True
                st.session_state.user_answer = ""
                st.experimental_rerun()  # Streamlitã®å†å®Ÿè¡Œã‚’ãƒˆãƒªã‚¬ãƒ¼

        st.write("ç¾åœ¨ã®ä¼šè©±å±¥æ­´:")
        st.write(st.session_state.conversation)

if __name__ == '__main__':
    main()