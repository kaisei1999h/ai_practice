import streamlit as st
import pdfplumber
import random
from langchain.chat_models import ChatOpenAI

# ãƒšãƒ¼ã‚¸ã®åˆæœŸåŒ–
def init_page():
    st.set_page_config(page_title="å­¦ç¿’ãƒ˜ãƒ«ãƒ‘ãƒ¼", page_icon="ğŸ“˜")
    st.title("PDF å­¦ç¿’æ”¯æ´ (GPT-4)")

# GPT-4ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
def initialize_model():
    return ChatOpenAI(temperature=0, model_name="gpt-4o")

# PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
def get_pdf_text(uploaded_file):
    with pdfplumber.open(uploaded_file) as pdf:
        return "\n".join(page.extract_text() for page in pdf.pages)

# PDFã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«ä¸€éƒ¨ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_random_text_sample(pdf_text, length=1000):
    start = random.randint(0, max(0, len(pdf_text) - length))
    return pdf_text[start:start + length]

# GPTã«å•é¡Œä½œæˆã‚’ä¾é ¼ã™ã‚‹é–¢æ•°
def generate_question_with_gpt(llm, pdf_text, question_type):
    text_sample = get_random_text_sample(pdf_text)
    prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«åŸºã¥ã„ã¦ã€{question_type}ã®ä¸€å•ä¸€ç­”å½¢å¼ã®å•é¡Œã‚’1ã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚
    å•é¡Œã¯ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã§ãã ã•ã„ã€‚å‰å›ã¨åŒã˜å•é¡Œã«ãªã‚‰ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚ã‚ãªãŸã¯å­¦æ ¡ã®å…ˆç”Ÿã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã¯æ—¥æœ¬ã®ä¸­å­¦ç”Ÿã§ã™ã€‚
    ç­”ãˆã¯åˆ¥é€”å‡ºåŠ›ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯å•é¡Œæ–‡ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

    ãƒ†ã‚­ã‚¹ãƒˆ:
    {text_sample}

    å•é¡Œã‚’å‡ºé¡Œã—ã¦ãã ã•ã„ã€‚
    """
    return llm.predict(prompt)

def check_answer_with_gpt(llm, conversation, user_answer):
    prompt = f"""ä»¥ä¸‹ã®ä¼šè©±ã®ç¶šãã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ã«å¯¾ã™ã‚‹æ­£èª¤åˆ¤å®šã¨è©³ç´°ãªè§£èª¬ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
    
    ã“ã‚Œã¾ã§ã®ä¼šè©±:
    {conversation}

    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”: {user_answer}

    æ­£èª¤åˆ¤å®šã¨è§£èª¬:
    """
    return llm.predict(prompt)

def generate_next_question(llm, conversation, pdf_text, question_type):
    text_sample = get_random_text_sample(pdf_text)
    prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«åŸºã¥ã„ã¦ã€{question_type}ã®ä¸€å•ä¸€ç­”å½¢å¼ã®å•é¡Œã‚’1ã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚
    å•é¡Œã¯ä¸€å•ã®ã¿ä½œæˆã—ã¦ãã ã•ã„ã€‚è¤‡æ•°ç­”ãˆã•ã›ã‚‹å•é¡Œã¯ä½œæˆã—ãªã„ã§ãã ã•ã„ã€‚å›ç­”ã¯ä¸€ã¤ã®ã¿ã«ã—ã¦ãã ã•ã„ã€‚
    å•é¡Œã¯ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã§ãã ã•ã„ã€‚å‰å›ã¨åŒã˜,ä¼¼ãŸã‚ˆã†ãªå•é¡Œã«ãªã‚‰ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
    ç­”ãˆã¯åˆ¥é€”å‡ºåŠ›ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯å•é¡Œæ–‡ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ã‚ãªãŸã¯å­¦æ ¡ã®å…ˆç”Ÿã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã¯æ—¥æœ¬ã®ä¸­å­¦ç”Ÿã§ã™ã€‚
    ã“ã‚Œã¾ã§ã®ä¼šè©±:
    {conversation}
    
    å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ:
    {text_sample}
    
    å•é¡Œã®ã‚¿ã‚¤ãƒ—: {question_type}
    
    æ–°ã—ã„å•é¡Œã‚’å‡ºé¡Œã—ã¦ãã ã•ã„ã€‚
    """
    return llm.predict(prompt)


def main():
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'initialized' not in st.session_state:
        st.session_state.initialized = True
        st.session_state.question_number = 1
        st.session_state.conversation = ""
        st.session_state.current_question = ""
        st.session_state.waiting_for_answer = False
        st.session_state.user_answer = ""
        st.session_state.pdf_text = ""
        st.session_state.question_type = ""

    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    llm = initialize_model()

    # PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader("å‹‰å¼·ç”¨PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ğŸ“š", type='pdf')
    if uploaded_file and not st.session_state.pdf_text:
        with st.spinner("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºä¸­..."):
            st.session_state.pdf_text = get_pdf_text(uploaded_file)
        st.success("PDFã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    # å•é¡Œã‚¿ã‚¤ãƒ—ã®å…¥åŠ›
    st.session_state.question_type = st.text_input("ã©ã®ã‚ˆã†ãªå•é¡Œã‚’å‡ºã—ã¦ã»ã—ã„ã§ã™ã‹ï¼Ÿ", value=st.session_state.question_type)
    
    # æœ€åˆã®å•é¡Œç”Ÿæˆ
    if st.session_state.pdf_text and st.session_state.question_type and not st.session_state.current_question:
        st.session_state.current_question = generate_question_with_gpt(llm, st.session_state.pdf_text, st.session_state.question_type)
        st.session_state.conversation += f"\nè³ªå• {st.session_state.question_number}: {st.session_state.current_question}"
        st.session_state.waiting_for_answer = True

    # ç¾åœ¨ã®å•é¡Œè¡¨ç¤º
    if st.session_state.current_question:
        st.write(f"å•é¡Œ {st.session_state.question_number}:")
        st.write(st.session_state.current_question)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”å…¥åŠ›ã¨è©•ä¾¡
    if st.session_state.waiting_for_answer:
        user_answer = st.text_input("ã‚ãªãŸã®å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", key=f"answer_input_{st.session_state.question_number}")
        if user_answer:
            st.session_state.conversation += f"\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”: {user_answer}"
            feedback = check_answer_with_gpt(llm, st.session_state.conversation, user_answer)
            st.write("è©•ä¾¡çµæœ:")
            st.write(feedback)
            st.session_state.conversation += f"\nè©•ä¾¡: {feedback}"
            st.session_state.waiting_for_answer = False
            st.session_state.user_answer = user_answer

    # æ¬¡ã®å•é¡Œã¸é€²ã‚€ãƒœã‚¿ãƒ³
    if not st.session_state.waiting_for_answer and st.session_state.current_question:
        if st.button("æ¬¡ã®å•é¡Œã¸é€²ã‚€", key=f"next_question_{st.session_state.question_number}"):
            st.session_state.question_number += 1
            st.session_state.current_question = generate_next_question(llm, st.session_state.conversation, st.session_state.pdf_text, st.session_state.question_type)
            st.session_state.conversation += f"\nè³ªå• {st.session_state.question_number}: {st.session_state.current_question}"
            st.session_state.waiting_for_answer = True
            st.session_state.user_answer = ""
            st.rerun()

    # # ä¼šè©±å±¥æ­´ã®è¡¨ç¤º
    # st.write("ç¾åœ¨ã®ä¼šè©±å±¥æ­´:")
    # st.write(st.session_state.conversation)

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤ï¼‰
    # st.write("Debug Info:")
    # st.write(f"Question Number: {st.session_state.question_number}")
    # st.write(f"Waiting for Answer: {st.session_state.waiting_for_answer}")
    # st.write(f"Current Question: {st.session_state.current_question}")

if __name__ == '__main__':
    main()